{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "! pip install pdfkit\n",
    "! pip install wkhtmltopdf\n",
    "https://wkhtmltopdf.org/downloads.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code made to save the knowledge that Mr. Gaves shares with us through his newsletter, Institut Des Libertés (https://institutdeslibertes.org/qui-sommes-nous/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Cartographie les articles\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import time\n",
    "\n",
    "def grab_data(page) :\n",
    "    session = requests.Session()\n",
    "    my_headers = {\"User-Agent\":\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/106.0.0.0 Safari/537.36\",\"Accept\":\"text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9\"}\n",
    "    url = \"https://institutdeslibertes.org/auteur/charlesgave/page/{}/\".format(page)\n",
    "    response = session.get(url, headers=my_headers)\n",
    "    html_soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    return html_soup\n",
    "\n",
    "def save_file(name, file) :\n",
    "    with open(\"{}.json\".format(name), \"w\") as write_file:\n",
    "        json.dump(file, write_file, indent=4)\n",
    "    print(\"Saved\")\n",
    "\n",
    "all_links = []\n",
    "for i in range(1,75) :  # 74 pages\n",
    "    print(i)\n",
    "    html_soup = grab_data(i)\n",
    "    articles = html_soup.find_all(class_=\"reports__card\")\n",
    "    for article in articles :\n",
    "        lien = article.find(\"a\", attrs={\"href\":True})[\"href\"]\n",
    "        date_cleared = slugify(article.contents[3].contents[1].text)\n",
    "        date_splited = date_cleared.split('-', 3)\n",
    "        date_converted = date_splited[2] + \"-\" + convert_month(date_splited[1]) + \"-\" + date_splited[0]\n",
    "        title_cleared = slugify(article.contents[3].contents[3].text)\n",
    "        file_name = date_converted + \"---\" + title_cleared\n",
    "        data = {\"filename\":file_name, \"link\":lien}\n",
    "        all_links.append(data)\n",
    "    time.sleep(0.1)\n",
    "    save_file(\"attempt_3\", all_links)\n",
    "\n",
    "all_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) grab article data et save html\n",
    "from bs4 import BeautifulSoup \n",
    "import requests\n",
    "import re\n",
    "from unidecode import unidecode\n",
    "import unicodedata\n",
    "\n",
    "def grab_data(url) :\n",
    "    session = requests.Session()\n",
    "    my_headers = {\"User-Agent\":\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/106.0.0.0 Safari/537.36\",\"Accept\":\"text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9\"}\n",
    "    response = session.get(url, headers=my_headers)\n",
    "    html_soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    return html_soup\n",
    "\n",
    "def slugify(value, allow_unicode=False):\n",
    "    value = str(value)\n",
    "    if allow_unicode:\n",
    "        value = unicodedata.normalize('NFKC', value)\n",
    "    else:\n",
    "        value = unicodedata.normalize('NFKD', value).encode('ascii', 'ignore').decode('ascii')\n",
    "    value = re.sub(r'[^\\w\\s-]', '', value.lower())\n",
    "    return re.sub(r'[-\\s]+', '-', value).strip('-_')\n",
    "\n",
    "def convert_month(month):\n",
    "    month_dict = {\n",
    "        \"janvier\": 1,\n",
    "        \"février\": 2,\n",
    "        \"mars\": 3,\n",
    "        \"avril\": 4,\n",
    "        \"mai\": 5,\n",
    "        \"juin\": 6,\n",
    "        \"juillet\": 7,\n",
    "        \"août\": 8,\n",
    "        \"septembre\": 9,\n",
    "        \"octobre\": 10,\n",
    "        \"novembre\": 11,\n",
    "        \"décembre\": 12\n",
    "    }\n",
    "    month = unidecode(month.lower())\n",
    "    for key in month_dict:\n",
    "        if unidecode(key).startswith(month):\n",
    "            return str(month_dict[key])\n",
    "    return None\n",
    "\n",
    "def work_in_progress(html_soup):\n",
    "    titre = html_soup.find(\"div\", {'class':'data-author'})\n",
    "    article = html_soup.find(\"div\", {'class':'data-text'})\n",
    "    \n",
    "    data = BeautifulSoup(\"<html><head><meta charset='utf-8'></head><body></body></html>\", \"html.parser\")\n",
    "    data.body.append(titre)\n",
    "    data.body.append(article)\n",
    "    \n",
    "    date_cleared = slugify(titre.contents[1].text)\n",
    "    date_splited = date_cleared.split('-', 3)\n",
    "    date_converted = date_splited[2] + \"-\" + convert_month(date_splited[1]) + \"-\" + date_splited[0]\n",
    "    title_cleared = slugify(titre.contents[3].text)\n",
    "    file_name = date_converted + \"---\" + title_cleared\n",
    "    return file_name, data\n",
    "\n",
    "def grab_process_save(url) :\n",
    "    html_soup = grab_data(url)\n",
    "    file_name, data = work_in_progress(html_soup)\n",
    "    with open(\"html//{}.html\".format(file_name), \"w\", encoding='utf-8') as file:\n",
    "        file.write(str(data.prettify()))\n",
    "\n",
    "for i in saucisse :\n",
    "    url = i[\"link\"]\n",
    "    grab_process_save(url)\n",
    "    print(\"done : \", i[\"filename\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3) Loop convert html to pdf\n",
    "import pdfkit\n",
    "import os\n",
    "path_wkhtmltopdf = r'C:\\Program Files\\wkhtmltopdf\\bin\\wkhtmltopdf.exe'\n",
    "config = pdfkit.configuration(wkhtmltopdf=path_wkhtmltopdf)\n",
    "directory = 'P:\\Code\\Prog\\IDL_PDF\\html'\n",
    "\n",
    "for html_name in os.listdir(directory):\n",
    "    if os.path.isfile('pdf\\{}.pdf'.format(html_name[:-5])) :\n",
    "        print(\"exist\")\n",
    "    else :\n",
    "        pdfkit.from_file('html\\{}'.format(html_name), 'pdf\\{}.pdf'.format(html_name[:-5]), configuration=config)\n",
    "        print(\"done added : \", html_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x) Load / Save json\n",
    "import os.path\n",
    "import json\n",
    "\n",
    "def save_file(name, file) :\n",
    "    with open(\"{}.json\".format(name), \"w\") as write_file:\n",
    "        json.dump(file, write_file, indent=4)\n",
    "    print(\"Saved\")\n",
    "\n",
    "def load_file(name) :\n",
    "    if os.path.isfile('{}.json'.format(name)) :\n",
    "        with open('{}.json'.format(name)) as json_file:\n",
    "            file = json.load(json_file)\n",
    "    else :\n",
    "        print(\"No previous data detected, initialisation\")\n",
    "        file = {}\n",
    "        save_file(name, file)\n",
    "        print(\"initialisation done\")\n",
    "    return file\n",
    "\n",
    "saucisse = load_file(\"all_links\")\n",
    "saucisse\n",
    "# save_file(\"save_1\", \"caca\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.9 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "978e1f1e844fa38cf76729a431b75a111107bbb8d7090e5ecb82e3ee11b3d754"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
